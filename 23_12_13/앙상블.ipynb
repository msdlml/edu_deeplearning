{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Loading\n",
    "iris = load_iris()\n",
    "\n",
    "# DataFrame으로 변환해서 처리하는게 쉽고 편해요!\n",
    "df = pd.DataFrame(iris.data,\n",
    "                  columns=iris.feature_names)\n",
    "\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "df['target'] = iris.target\n",
    "# display(df)\n",
    "\n",
    "# 결측치와 이상치는 없다고 가정하고 진행!\n",
    "# 중복데이터 처리\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 이제 x_data와 t_data를 추출하면 될 거 같아요!\n",
    "# x_data는 4개의 feature\n",
    "x_data = df.drop(['target'],\n",
    "                 axis=1,\n",
    "                 inplace=False).values\n",
    "t_data = df['target'].values\n",
    "\n",
    "# 데이터 분리보다 정규화를 먼저 진행하는게 조금 더 편해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# 데이터 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn 정확도 : 0.9777777777777777\n",
      "svm 정확도 : 0.9777777777777777\n",
      "결정트리 정확도 : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_data_train_norm,t_data_train)\n",
    "knn_acc = accuracy_score(t_data_test,knn.predict(x_data_test_norm))\n",
    "print(f'knn 정확도 : {knn_acc}')\n",
    "\n",
    "svm = SVC(kernel='linear',C = 0.5,probability=True)\n",
    "svm.fit(x_data_train_norm,t_data_train)\n",
    "svm_acc = accuracy_score(t_data_test,svm.predict(x_data_test_norm))\n",
    "print(f'svm 정확도 : {svm_acc}')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_data_train_norm,t_data_train)\n",
    "tree_acc = accuracy_score(t_data_test,tree.predict(x_data_test_norm))\n",
    "print(f'결정트리 정확도 : {tree_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 모델(hard voting)의 accuracy : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "#앙상블 모델만들기\n",
    "# hard voting classifier(hvc)\n",
    "\n",
    "hvc = VotingClassifier(estimators=[('KNN',knn),\n",
    "                                   ('SVM',svm),\n",
    "                                   ('DT',tree)],\n",
    "                                   voting='hard')\n",
    "hvc.fit(x_data_train_norm,t_data_train)\n",
    "hvc_acc = accuracy_score(t_data_test, hvc.predict(x_data_test_norm))\n",
    "print(f'앙상블 모델(hard voting)의 accuracy : {hvc_acc}')  # 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 모델(hard voting)의 accuracy : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "hvc = VotingClassifier(estimators=[('KNN',knn),\n",
    "                                   ('SVM',svm),\n",
    "                                   ('DT',tree)],\n",
    "                                   voting='soft')\n",
    "hvc.fit(x_data_train_norm,t_data_train)\n",
    "hvc_acc = accuracy_score(t_data_test, hvc.predict(x_data_test_norm))\n",
    "print(f'앙상블 모델(hard voting)의 accuracy : {hvc_acc}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 모델의 accuracy : 0.9555555555555556\n",
      "Decision Tree 모델의 accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "#bagging\n",
    "# 앙상블 Bagging을 구현해 보아요!\n",
    "# Decision Tree를 모아서 만든 Random Forest를 구현해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "iris = load_iris()\n",
    "\n",
    "# DataFrame으로 변환해서 처리하는게 쉽고 편해요!\n",
    "df = pd.DataFrame(iris.data,\n",
    "                  columns=iris.feature_names)\n",
    "\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "df['target'] = iris.target\n",
    "# display(df)\n",
    "\n",
    "# 결측치와 이상치는 없다고 가정하고 진행!\n",
    "# 중복데이터 처리\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 이제 x_data와 t_data를 추출하면 될 거 같아요!\n",
    "# x_data는 4개의 feature\n",
    "x_data = df.drop(['target'],\n",
    "                 axis=1,\n",
    "                 inplace=False).values\n",
    "t_data = df['target'].values\n",
    "\n",
    "# 데이터 분리보다 정규화를 먼저 진행하는게 조금 더 편해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# 데이터 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_data_train_norm, t_data_train)\n",
    "dt_acc = accuracy_score(t_data_test, dt.predict(x_data_test_norm))\n",
    "print(f'Decision Tree 모델의 accuracy : {dt_acc}')  # 0.97\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=50,\n",
    "                             max_depth = 3,\n",
    "                             random_state=20)\n",
    "rfc.fit(x_data_train_norm, t_data_train)\n",
    "rfc_acc = accuracy_score(t_data_test, rfc.predict(x_data_test_norm))\n",
    "print(f'Decision Tree 모델의 accuracy : {rfc_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb모델의 accuracy : 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Boosting\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 50,\n",
    "                    max_depth = 3,\n",
    "                    random_state = 20)\n",
    "xgb.fit(x_data_train_norm,t_data_train)\n",
    "xgb_acc = accuracy_score(t_data_test,xgb.predict(x_data_test_norm))\n",
    "print(f'xgb모델의 accuracy : {xgb_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 237ms/step - loss: 0.6931 - acc: 0.5000\n",
      "정확도는 : [0.6931471824645996, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# 최초의 neural network => Perceptron\n",
    "# 아주 간단하게 생각하면 logistic regression과 같아요!\n",
    "# 대신에 activation 함수를 logistic은 sigmoid를 이용해요\n",
    "# 하지만 Perceptron은 activation함수로 step function을 사용해요!\n",
    "\n",
    "# 이런 Perceptron은 각종 GATE연산을 학습할 수 있으면\n",
    "# 이를 이용해서 AI를 만들 수 있겠다라고 당시에 생각했어요!\n",
    "# 여기서 말하는 GATE연산은(AND, OR, NOR, XOR, ...)\n",
    "\n",
    "# 우리도 Perceptron이 GATE연산을 학습할 수 있는지 확인하기 위해\n",
    "# Logistic Regression을 이용해서 GATE연산을 학습해 볼꺼예요!\n",
    "\n",
    "# Tensorflow Keras로 구현해 보아요!\n",
    "# AND, OR, XOR 연산만 해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Training Data Set\n",
    "# AND GET에 대한 데이터\n",
    "x_data = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]], dtype=np.float32)\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (2,)))\n",
    "model.add(Dense(units =1,\n",
    "                activation = 'sigmoid'))\n",
    "model.compile(optimizer = Adam(learning_rate = 1e-2),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_data,\n",
    "          t_data,\n",
    "          epochs= 10000,\n",
    "          verbose=0)\n",
    "print(f'정확도는 : {model.evaluate(x_data,t_data)}')\n",
    "#[0.0003830736386589706, 1.0]  => AND연산에 대한 data로 학습\n",
    "#[0.00019464879005681723, 1.0] => OR 연산에 대한 data로 학습\n",
    "#[0.6931471824645996, 0.5] => XOR연산에 대한 datd로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5160e-08 - acc: 1.0000\n",
      "정확도는 : [1.5159578481416247e-08, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 다중 layer perceptron을 이용해서 GATE연산을 학습해 볼꺼예요!\n",
    "\n",
    "# Tensorflow Keras로 구현해 보아요!\n",
    "# AND, OR, XOR 연산만 해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]], dtype=np.float32)\n",
    "# AND GATE에 대한 데이터\n",
    "# t_data = np.array([[0],[0],[0],[1]], dtype=np.float32)\n",
    "# OR GATE에 대한 데이터\n",
    "# t_data = np.array([[0],[1],[1],[1]], dtype=np.float32)\n",
    "# XOR GATE에 대한 데이터\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(2,)))  # Input Layer\n",
    "\n",
    "# 여러개의 Hidden Layer\n",
    "# Hidden Layer는 Dense layer를 사용해요!\n",
    "model.add(Dense(units=10,\n",
    "                activation='sigmoid'))\n",
    "model.add(Dense(units=6,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=1,               # Output Layer\n",
    "                activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_data,\n",
    "          t_data,\n",
    "          epochs=30000,\n",
    "          verbose=0)\n",
    "\n",
    "print(f'정확도는 : {model.evaluate(x_data,t_data)}')\n",
    "# 정확도는 : [0.0003516915312502533, 1.0] => AND 연산에 대한 학습 결과\n",
    "# 정확도는 : [0.00015051690570544451, 1.0] => OR 연산에 대한 학습 결과\n",
    "# 정확도는 : [0.6931471824645996, 0.75] => XOR 연산에 대한 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 실습에서 사용했던 MNIST 데이터셋을\n",
    "# 두가지 방식으로 구현해 볼꺼예요!\n",
    "\n",
    "# 1. Logistic Regression을 이용해서 Multinomial Classification\n",
    "# 2. DNN으로 Multinomial Classification 구현을 해 볼꺼예요!\n",
    "\n",
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4184 - acc: 0.8784 - val_loss: 0.3031 - val_acc: 0.9148\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2979 - acc: 0.9153 - val_loss: 0.3189 - val_acc: 0.9073\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2789 - acc: 0.9210 - val_loss: 0.2862 - val_acc: 0.9180\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2643 - acc: 0.9248 - val_loss: 0.2956 - val_acc: 0.9155\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2564 - acc: 0.9259 - val_loss: 0.2886 - val_acc: 0.9168\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2465 - acc: 0.9293 - val_loss: 0.3030 - val_acc: 0.9143\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2412 - acc: 0.9302 - val_loss: 0.2942 - val_acc: 0.9185\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2356 - acc: 0.9308 - val_loss: 0.3031 - val_acc: 0.9167\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2349 - acc: 0.9318 - val_loss: 0.2990 - val_acc: 0.9162\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2291 - acc: 0.9339 - val_loss: 0.3146 - val_acc: 0.9121\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2239 - acc: 0.9351 - val_loss: 0.3126 - val_acc: 0.9146\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2296 - acc: 0.9321 - val_loss: 0.3161 - val_acc: 0.9165\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2198 - acc: 0.9358 - val_loss: 0.3244 - val_acc: 0.9075\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2175 - acc: 0.9356 - val_loss: 0.3176 - val_acc: 0.9150\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2172 - acc: 0.9356 - val_loss: 0.3436 - val_acc: 0.9046\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2181 - acc: 0.9347 - val_loss: 0.3377 - val_acc: 0.9097\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2117 - acc: 0.9385 - val_loss: 0.3352 - val_acc: 0.9112\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2115 - acc: 0.9380 - val_loss: 0.3282 - val_acc: 0.9119\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2119 - acc: 0.9375 - val_loss: 0.3238 - val_acc: 0.9150\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2058 - acc: 0.9394 - val_loss: 0.3346 - val_acc: 0.9124\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2066 - acc: 0.9395 - val_loss: 0.3394 - val_acc: 0.9088\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2045 - acc: 0.9406 - val_loss: 0.3414 - val_acc: 0.9128\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2036 - acc: 0.9402 - val_loss: 0.3378 - val_acc: 0.9145\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2012 - acc: 0.9397 - val_loss: 0.3630 - val_acc: 0.9044\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2056 - acc: 0.9394 - val_loss: 0.3745 - val_acc: 0.9034\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.2003 - acc: 0.9408 - val_loss: 0.3408 - val_acc: 0.9133\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.1989 - acc: 0.9413 - val_loss: 0.3630 - val_acc: 0.9092\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.1989 - acc: 0.9412 - val_loss: 0.3579 - val_acc: 0.9104\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.1910 - acc: 0.9423 - val_loss: 0.3839 - val_acc: 0.9071\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.1946 - acc: 0.9412 - val_loss: 0.3591 - val_acc: 0.9090\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.1966 - acc: 0.9416 - val_loss: 0.3548 - val_acc: 0.9129\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1942 - acc: 0.9420 - val_loss: 0.3693 - val_acc: 0.9082\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1973 - acc: 0.9406 - val_loss: 0.3709 - val_acc: 0.9075\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1916 - acc: 0.9429 - val_loss: 0.3756 - val_acc: 0.9066\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1906 - acc: 0.9449 - val_loss: 0.3762 - val_acc: 0.9107\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1886 - acc: 0.9443 - val_loss: 0.3902 - val_acc: 0.9043\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1881 - acc: 0.9443 - val_loss: 0.3647 - val_acc: 0.9111\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1860 - acc: 0.9444 - val_loss: 0.3708 - val_acc: 0.9060\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1865 - acc: 0.9434 - val_loss: 0.3760 - val_acc: 0.9085\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1855 - acc: 0.9437 - val_loss: 0.3954 - val_acc: 0.9063\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1912 - acc: 0.9432 - val_loss: 0.3781 - val_acc: 0.9094\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1832 - acc: 0.9479 - val_loss: 0.4098 - val_acc: 0.9002\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1853 - acc: 0.9438 - val_loss: 0.3882 - val_acc: 0.9088\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1885 - acc: 0.9443 - val_loss: 0.4057 - val_acc: 0.9007\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1849 - acc: 0.9447 - val_loss: 0.3992 - val_acc: 0.9043\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1810 - acc: 0.9469 - val_loss: 0.4091 - val_acc: 0.9036\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1827 - acc: 0.9460 - val_loss: 0.3894 - val_acc: 0.9075\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1836 - acc: 0.9456 - val_loss: 0.4036 - val_acc: 0.9063\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1827 - acc: 0.9474 - val_loss: 0.3955 - val_acc: 0.9075\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1788 - acc: 0.9464 - val_loss: 0.3958 - val_acc: 0.9053\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1789 - acc: 0.9457 - val_loss: 0.4135 - val_acc: 0.9037\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1790 - acc: 0.9478 - val_loss: 0.4050 - val_acc: 0.9058\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1819 - acc: 0.9458 - val_loss: 0.3983 - val_acc: 0.9065\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1772 - acc: 0.9452 - val_loss: 0.4024 - val_acc: 0.9060\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1769 - acc: 0.9477 - val_loss: 0.4083 - val_acc: 0.9073\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1770 - acc: 0.9476 - val_loss: 0.4128 - val_acc: 0.9063\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1784 - acc: 0.9454 - val_loss: 0.4048 - val_acc: 0.9043\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1785 - acc: 0.9464 - val_loss: 0.4109 - val_acc: 0.9088\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1743 - acc: 0.9475 - val_loss: 0.4393 - val_acc: 0.8939\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1777 - acc: 0.9470 - val_loss: 0.4286 - val_acc: 0.9020\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1710 - acc: 0.9489 - val_loss: 0.4130 - val_acc: 0.9073\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1719 - acc: 0.9481 - val_loss: 0.4191 - val_acc: 0.9073\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1734 - acc: 0.9484 - val_loss: 0.4205 - val_acc: 0.9061\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1733 - acc: 0.9475 - val_loss: 0.4238 - val_acc: 0.9039\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1760 - acc: 0.9470 - val_loss: 0.4280 - val_acc: 0.9071\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1760 - acc: 0.9478 - val_loss: 0.4360 - val_acc: 0.9024\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1725 - acc: 0.9491 - val_loss: 0.4200 - val_acc: 0.9051\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1729 - acc: 0.9475 - val_loss: 0.4230 - val_acc: 0.9041\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1691 - acc: 0.9491 - val_loss: 0.4231 - val_acc: 0.9073\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1733 - acc: 0.9480 - val_loss: 0.4260 - val_acc: 0.9063\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1688 - acc: 0.9499 - val_loss: 0.4280 - val_acc: 0.9022\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1679 - acc: 0.9514 - val_loss: 0.4447 - val_acc: 0.9037\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1697 - acc: 0.9489 - val_loss: 0.4420 - val_acc: 0.9010\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1654 - acc: 0.9505 - val_loss: 0.4516 - val_acc: 0.9005\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1709 - acc: 0.9480 - val_loss: 0.4444 - val_acc: 0.9029\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1707 - acc: 0.9478 - val_loss: 0.4426 - val_acc: 0.9061\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1699 - acc: 0.9503 - val_loss: 0.4543 - val_acc: 0.9009\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1685 - acc: 0.9491 - val_loss: 0.4442 - val_acc: 0.9075\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1688 - acc: 0.9496 - val_loss: 0.4555 - val_acc: 0.9014\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1683 - acc: 0.9486 - val_loss: 0.4435 - val_acc: 0.9027\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1687 - acc: 0.9497 - val_loss: 0.4463 - val_acc: 0.9041\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1640 - acc: 0.9508 - val_loss: 0.4552 - val_acc: 0.9048\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1678 - acc: 0.9489 - val_loss: 0.4653 - val_acc: 0.8995\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1676 - acc: 0.9490 - val_loss: 0.4586 - val_acc: 0.9027\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1662 - acc: 0.9511 - val_loss: 0.4629 - val_acc: 0.9020\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1672 - acc: 0.9506 - val_loss: 0.4525 - val_acc: 0.9046\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1682 - acc: 0.9500 - val_loss: 0.4666 - val_acc: 0.9020\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1682 - acc: 0.9504 - val_loss: 0.4642 - val_acc: 0.9051\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1661 - acc: 0.9502 - val_loss: 0.4661 - val_acc: 0.9020\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1660 - acc: 0.9498 - val_loss: 0.4753 - val_acc: 0.9036\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1639 - acc: 0.9503 - val_loss: 0.4689 - val_acc: 0.9036\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1632 - acc: 0.9518 - val_loss: 0.4687 - val_acc: 0.9009\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1626 - acc: 0.9514 - val_loss: 0.4771 - val_acc: 0.8998\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1607 - acc: 0.9511 - val_loss: 0.4808 - val_acc: 0.8985\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1640 - acc: 0.9493 - val_loss: 0.4742 - val_acc: 0.9031\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1639 - acc: 0.9493 - val_loss: 0.4923 - val_acc: 0.8993\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1639 - acc: 0.9499 - val_loss: 0.4792 - val_acc: 0.9020\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1560 - acc: 0.9535 - val_loss: 0.4960 - val_acc: 0.8974\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1621 - acc: 0.9515 - val_loss: 0.4844 - val_acc: 0.8980\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.1627 - acc: 0.9512 - val_loss: 0.4916 - val_acc: 0.9015\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "x_data = df.drop('label',axis=1,inplace=False).values\n",
    "t_data = df['label'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "x_data_train_norm,x_data_test_norm,t_data_train,t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# model 구현(regression model 구현)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (784,)))\n",
    "\n",
    "model.add(Dense(units = 10,\n",
    "                activation = 'softmax'))\n",
    "model.compile(optimizer = Adam(learning_rate= 1e-2),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_data_train_norm,t_data_train,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 100,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 1s 2ms/step - loss: 0.5360 - acc: 0.8950\n",
      "[0.5360375046730042, 0.8949999809265137]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      1240\n",
      "           1       0.97      0.95      0.96      1405\n",
      "           2       0.89      0.85      0.87      1253\n",
      "           3       0.87      0.87      0.87      1305\n",
      "           4       0.93      0.89      0.91      1222\n",
      "           5       0.89      0.77      0.82      1139\n",
      "           6       0.91      0.95      0.93      1241\n",
      "           7       0.88      0.93      0.91      1320\n",
      "           8       0.80      0.90      0.85      1219\n",
      "           9       0.86      0.88      0.87      1256\n",
      "\n",
      "    accuracy                           0.90     12600\n",
      "   macro avg       0.90      0.89      0.89     12600\n",
      "weighted avg       0.90      0.90      0.89     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "predict = tf.argmax(model.predict(x_data_test_norm),axis=1).numpy()\n",
    "print(model.evaluate(x_data_test_norm,t_data_test))\n",
    "print(classification_report(t_data_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.2947 - acc: 0.9088 - val_loss: 0.1838 - val_acc: 0.9440\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.1438 - acc: 0.9569 - val_loss: 0.1711 - val_acc: 0.9491\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.1251 - acc: 0.9635 - val_loss: 0.1723 - val_acc: 0.9512\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.1042 - acc: 0.9693 - val_loss: 0.1486 - val_acc: 0.9575\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0920 - acc: 0.9722 - val_loss: 0.1836 - val_acc: 0.9551\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0833 - acc: 0.9758 - val_loss: 0.1638 - val_acc: 0.9571\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0679 - acc: 0.9803 - val_loss: 0.1513 - val_acc: 0.9633\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0734 - acc: 0.9794 - val_loss: 0.1836 - val_acc: 0.9594\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0726 - acc: 0.9799 - val_loss: 0.2212 - val_acc: 0.9573\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0718 - acc: 0.9798 - val_loss: 0.1885 - val_acc: 0.9585\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0545 - acc: 0.9838 - val_loss: 0.1646 - val_acc: 0.9648\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0738 - acc: 0.9816 - val_loss: 0.2095 - val_acc: 0.9633\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0767 - acc: 0.9809 - val_loss: 0.2391 - val_acc: 0.9546\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0652 - acc: 0.9829 - val_loss: 0.2047 - val_acc: 0.9624\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0437 - acc: 0.9886 - val_loss: 0.1712 - val_acc: 0.9665\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0375 - acc: 0.9900 - val_loss: 0.2426 - val_acc: 0.9651\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0619 - acc: 0.9872 - val_loss: 0.2458 - val_acc: 0.9636\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0518 - acc: 0.9875 - val_loss: 0.2568 - val_acc: 0.9639\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0453 - acc: 0.9894 - val_loss: 0.2147 - val_acc: 0.9668\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0531 - acc: 0.9880 - val_loss: 0.2861 - val_acc: 0.9634\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0638 - acc: 0.9856 - val_loss: 0.2694 - val_acc: 0.9617\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0447 - acc: 0.9899 - val_loss: 0.2656 - val_acc: 0.9663\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0700 - acc: 0.9866 - val_loss: 0.2880 - val_acc: 0.9626\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0564 - acc: 0.9878 - val_loss: 0.2526 - val_acc: 0.9651\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0283 - acc: 0.9935 - val_loss: 0.2696 - val_acc: 0.9696\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0344 - acc: 0.9929 - val_loss: 0.2420 - val_acc: 0.9690\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0260 - acc: 0.9944 - val_loss: 0.2926 - val_acc: 0.9655\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0738 - acc: 0.9876 - val_loss: 0.3142 - val_acc: 0.9600\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0724 - acc: 0.9856 - val_loss: 0.2841 - val_acc: 0.9639\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0459 - acc: 0.9902 - val_loss: 0.2739 - val_acc: 0.9643\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0309 - acc: 0.9928 - val_loss: 0.2834 - val_acc: 0.9696\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0364 - acc: 0.9926 - val_loss: 0.4394 - val_acc: 0.9624\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0668 - acc: 0.9883 - val_loss: 0.3327 - val_acc: 0.9633\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0473 - acc: 0.9915 - val_loss: 0.2524 - val_acc: 0.9679\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0351 - acc: 0.9935 - val_loss: 0.2768 - val_acc: 0.9696\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0318 - acc: 0.9946 - val_loss: 0.2910 - val_acc: 0.9687\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0152 - acc: 0.9967 - val_loss: 0.3377 - val_acc: 0.9704\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0571 - acc: 0.9899 - val_loss: 0.4783 - val_acc: 0.9571\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0582 - acc: 0.9904 - val_loss: 0.5590 - val_acc: 0.9614\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0510 - acc: 0.9901 - val_loss: 0.3937 - val_acc: 0.9646\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0375 - acc: 0.9920 - val_loss: 0.3505 - val_acc: 0.9696\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0504 - acc: 0.9915 - val_loss: 0.3624 - val_acc: 0.9679\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0253 - acc: 0.9951 - val_loss: 0.3827 - val_acc: 0.9680\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0200 - acc: 0.9955 - val_loss: 0.3614 - val_acc: 0.9656\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0240 - acc: 0.9951 - val_loss: 0.4738 - val_acc: 0.9673\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.3363 - val_acc: 0.9677\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0445 - acc: 0.9928 - val_loss: 0.4654 - val_acc: 0.9600\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0657 - acc: 0.9886 - val_loss: 0.3304 - val_acc: 0.9685\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0399 - acc: 0.9934 - val_loss: 0.3481 - val_acc: 0.9673\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0274 - acc: 0.9954 - val_loss: 0.3670 - val_acc: 0.9714\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0403 - acc: 0.9947 - val_loss: 0.4950 - val_acc: 0.9648\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0563 - acc: 0.9918 - val_loss: 0.4743 - val_acc: 0.9696\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0226 - acc: 0.9960 - val_loss: 0.3985 - val_acc: 0.9706\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0470 - acc: 0.9949 - val_loss: 0.4927 - val_acc: 0.9605\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0663 - acc: 0.9915 - val_loss: 0.4624 - val_acc: 0.9634\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0362 - acc: 0.9932 - val_loss: 0.4386 - val_acc: 0.9673\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0489 - acc: 0.9929 - val_loss: 0.5231 - val_acc: 0.9650\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0483 - acc: 0.9913 - val_loss: 0.5565 - val_acc: 0.9612\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0449 - acc: 0.9923 - val_loss: 0.4070 - val_acc: 0.9672\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0260 - acc: 0.9948 - val_loss: 0.4066 - val_acc: 0.9707\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0177 - acc: 0.9967 - val_loss: 0.4186 - val_acc: 0.9707\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0152 - acc: 0.9973 - val_loss: 0.5451 - val_acc: 0.9680\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0341 - acc: 0.9945 - val_loss: 0.4028 - val_acc: 0.9702\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0146 - acc: 0.9968 - val_loss: 0.3949 - val_acc: 0.9752\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0115 - acc: 0.9977 - val_loss: 0.5764 - val_acc: 0.9723\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0365 - acc: 0.9945 - val_loss: 0.5653 - val_acc: 0.9687\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0525 - acc: 0.9924 - val_loss: 0.4530 - val_acc: 0.9684\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0513 - acc: 0.9907 - val_loss: 0.4757 - val_acc: 0.9568\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.4173 - val_acc: 0.9662\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0284 - acc: 0.9943 - val_loss: 0.4647 - val_acc: 0.9684\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0322 - acc: 0.9949 - val_loss: 0.5713 - val_acc: 0.9665\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0556 - acc: 0.9920 - val_loss: 0.4688 - val_acc: 0.9679\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0259 - acc: 0.9957 - val_loss: 0.5979 - val_acc: 0.9653\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0328 - acc: 0.9946 - val_loss: 0.4800 - val_acc: 0.9690\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0667 - acc: 0.9923 - val_loss: 0.4975 - val_acc: 0.9639\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0353 - acc: 0.9939 - val_loss: 0.4696 - val_acc: 0.9675\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0314 - acc: 0.9953 - val_loss: 0.5285 - val_acc: 0.9672\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0242 - acc: 0.9957 - val_loss: 0.5004 - val_acc: 0.9697\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0122 - acc: 0.9975 - val_loss: 0.5045 - val_acc: 0.9655\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.5627 - val_acc: 0.9682\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0412 - acc: 0.9943 - val_loss: 0.6595 - val_acc: 0.9648\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0624 - acc: 0.9923 - val_loss: 0.6996 - val_acc: 0.9697\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0512 - acc: 0.9939 - val_loss: 0.5395 - val_acc: 0.9697\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0264 - acc: 0.9955 - val_loss: 0.5711 - val_acc: 0.9672\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0403 - acc: 0.9931 - val_loss: 0.6140 - val_acc: 0.9679\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0336 - acc: 0.9948 - val_loss: 0.6784 - val_acc: 0.9667\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0298 - acc: 0.9959 - val_loss: 0.6893 - val_acc: 0.9672\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0680 - acc: 0.9940 - val_loss: 0.8012 - val_acc: 0.9629\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0461 - acc: 0.9941 - val_loss: 0.9043 - val_acc: 0.9662\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0500 - acc: 0.9934 - val_loss: 0.4728 - val_acc: 0.9667\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0378 - acc: 0.9945 - val_loss: 0.5290 - val_acc: 0.9646\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0235 - acc: 0.9960 - val_loss: 0.4724 - val_acc: 0.9684\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0230 - acc: 0.9960 - val_loss: 0.5356 - val_acc: 0.9714\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0140 - acc: 0.9973 - val_loss: 0.5669 - val_acc: 0.9709\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0382 - acc: 0.9952 - val_loss: 0.6844 - val_acc: 0.9670\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0244 - acc: 0.9968 - val_loss: 0.5609 - val_acc: 0.9724\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0187 - acc: 0.9968 - val_loss: 0.4695 - val_acc: 0.9662\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0186 - acc: 0.9962 - val_loss: 0.6284 - val_acc: 0.9694\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0282 - acc: 0.9962 - val_loss: 0.5336 - val_acc: 0.9658\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0338 - acc: 0.9954 - val_loss: 0.5863 - val_acc: 0.9694\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (784,)))\n",
    "model.add(Dense(units = 256,\n",
    "                activation = 'relu'))\n",
    "model.add(Dense(units = 128,\n",
    "                activation = 'relu'))\n",
    "\n",
    "model.add(Dense(units = 10,\n",
    "                activation = 'softmax'))\n",
    "model.compile(optimizer = Adam(learning_rate= 1e-2),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_data_train_norm,t_data_train,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 100,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose = 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 1s 2ms/step - loss: 0.8129 - acc: 0.9659\n",
      "[0.8129140138626099, 0.9658730030059814]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1240\n",
      "           1       0.98      0.98      0.98      1405\n",
      "           2       0.97      0.97      0.97      1253\n",
      "           3       0.97      0.96      0.96      1305\n",
      "           4       0.96      0.97      0.96      1222\n",
      "           5       0.96      0.95      0.96      1139\n",
      "           6       0.98      0.97      0.98      1241\n",
      "           7       0.98      0.97      0.97      1320\n",
      "           8       0.90      0.97      0.93      1219\n",
      "           9       0.97      0.93      0.95      1256\n",
      "\n",
      "    accuracy                           0.97     12600\n",
      "   macro avg       0.97      0.97      0.97     12600\n",
      "weighted avg       0.97      0.97      0.97     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = tf.argmax(model.predict(x_data_test_norm),axis=1).numpy()\n",
    "print(model.evaluate(x_data_test_norm,t_data_test))\n",
    "#[0.6647006273269653, 0.9633333086967468]\n",
    "print(classification_report(t_data_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'],color ='r')\n",
    "plt.plot(history.history['val_loss'],color ='b')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.3084 - acc: 0.9040 - val_loss: 0.2399 - val_acc: 0.9274\n",
      "Epoch 2/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.1596 - acc: 0.9517 - val_loss: 0.1690 - val_acc: 0.9474\n",
      "Epoch 3/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.1158 - acc: 0.9633 - val_loss: 0.1476 - val_acc: 0.9571\n",
      "Epoch 4/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0956 - acc: 0.9703 - val_loss: 0.1901 - val_acc: 0.9474\n",
      "Epoch 5/1000\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0884 - acc: 0.9725 - val_loss: 0.1750 - val_acc: 0.9594\n",
      "Epoch 6/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0762 - acc: 0.9769 - val_loss: 0.1647 - val_acc: 0.9641\n",
      "Epoch 7/1000\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.0948 - acc: 0.9736 - val_loss: 0.1761 - val_acc: 0.9600\n",
      "Epoch 8/1000\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.0785 - acc: 0.9780 - val_loss: 0.1761 - val_acc: 0.9633\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 0.1757 - acc: 0.9529\n",
      "[0.17569197714328766, 0.9528571367263794]\n"
     ]
    }
   ],
   "source": [
    "#케라스의 조기종료 기능\n",
    "\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # 원래 one-hot 처리를 해야 해요!\n",
    "                             # 하지만 keras에게 one-hot처리를 위임할 수\n",
    "                             # 있어서 따로 처리는 안할꺼예요!\n",
    "# 정규화는 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train, test 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# Model 구현(Regression Model 구현)\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Flatten(input_shape=(784,)))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(units=256,\n",
    "                activation='relu'))\n",
    "model.add(Dense(units=128,\n",
    "                activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Early Stopping Callback 설정\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# monitor : 조기 종료의 기준\n",
    "# patience : loss값이 중가하는걸 몇번참을것인가에 대한 숫자.\n",
    "# 1 epoch val_loss : 0.1\n",
    "# 2 epoch val_loss : 0.05\n",
    "# 3 epoch val_loss : 0.04\n",
    "# 4 epoch val_loss : 0.06   - 1번 참자!!\n",
    "# 5 epoch val_loss : 0.05   - 2번 참자!!\n",
    "# 6 epoch val_loss : 0.03\n",
    "es_cb = EarlyStopping(monitor='val_loss',\n",
    "                      patience=5,\n",
    "                      restore_best_weights=True)\n",
    "\n",
    "# model.summary()\n",
    "# 모델 학습\n",
    "history = model.fit(x_data_train_norm,\n",
    "                    t_data_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[es_cb],\n",
    "                    verbose=1)\n",
    "# 모델 평가\n",
    "print(model.evaluate(x_data_test_norm,\n",
    "                     t_data_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
